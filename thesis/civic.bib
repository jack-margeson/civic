@article{Anderson2020,
  author   = {Anderson,David P.},
  year     = {2020},
  title    = {BOINC: A Platform for Volunteer Computing},
  journal  = {Journal of grid computing},
  volume   = {18},
  number   = {1},
  pages    = {99-122},
  abstract = {“Volunteer computing” is the use of consumer digital devices for high-throughput scientific computing. It can provide large computing capacity at low cost, but presents challenges due to device heterogeneity, unreliability, and churn. BOINC, a widely-used open-source middleware system for volunteer computing, addresses these challenges. We describe BOINC’s features, architecture, implementation, and algorithms.},
  keywords = {Algorithms; BOINC; Computer Science; Computer Science, Information Systems; Computer Science, Theory & Methods; Distributed computing; Heterogeneity; High-throughput computing; Management of Computing and Information Systems; Middleware; Processor Architectures; Science & Technology; Scientific computing; Technology; User Interfaces and Human Computer Interaction; Volunteer computing},
  isbn     = {1570-7873},
  language = {English}
}

@article{Cappello2005,
  author   = {Cappello,Franck and Djilali,Samir and Fedak,Gilles and Herault,Thomas and Magniette,Frédéric and Néri,Vincent and Lodygensky,Oleg},
  year     = {2005},
  title    = {Computing on large-scale distributed systems: XtremWeb architecture, programming models, security, tests and convergence with grid},
  journal  = {Future generation computer systems},
  volume   = {21},
  number   = {3},
  pages    = {417-437},
  abstract = {Global Computing systems belong to the class of large-scale distributed systems. Their properties high computational, storage and communication performance potentials, high resilience make them attractive in academia and industry as computing infrastructures in complement to more classical infrastructures such as clusters or supercomputers. However, generalizing the use of these systems in a multi-user and multi-parallel programming context involves finding solutions and providing mechanisms for many issues such as programming bag of tasks and message passing parallel applications, securing the application, the system itself and the computing nodes, deploying the systems for harnessing resources managed in different ways. In this paper, we present our research, often influenced by user demands, towards a Computational peer-to- peer system called XtremWeb. We describe (a) the architecture of the system and its motivations, (b) the parallel programming paradigms available in XtremWeb and how they are implemented, (c) the deployment issues and what mechanisms are used to harness simultaneously uncoordinated set of resources, and resources managed by batch schedulers and (d) the security issue and how we address, inside XtremWeb, the protection of the computing resources. We present two multi-parametric applications to be used in production: Aires belonging to the high energy physics (HEP) Auger project and a protein conformation predictor using a molecular dynamic simulator. To evaluate the performance and volatility tolerance, we present experiment results for bag of tasks applications and message passing applications. We show that the system can tolerate massive failure and we discuss the performance of the node protection mechanism. Based on the XtremWeb project developments and evolutions, we will discuss the convergence between Global Computing systems and Grid.;Global Computing systems belong to the class of large-scale distributed systems. Their properties high computational. storage and communication performance potentials, high resilience make them attractive in academia and industry as computing infrastructures in complement to more classical infrastructures such as clusters or supercomputers. However, generalizing the use of these systems in a multi-user and multi-parallel programming context involves finding solutions and providing mechanisms for many issues such as programming bag of tasks and message passing parallel applications. securing the application, the system itself and the computing nodes, deploying the systems for harnessing resources managed in different ways. In this paper, we present our research, often influenced by user demands. towards a Computational peer-to-peer system called XtremWeb. We describe (a) the architecture of the system and its motivations, (b) the parallel programming paradigms available in XtremWeb and how they are implemented, (c) the deployment issues and what mechanisms are used to harness simultaneously uncoordinated set of resources, and resources managed by batch schedulers and (d) the security issue and how we address, inside XtremWeb. the protection of the computing resources. We present two multi-parametric applications to be used in production: Aires belonging to the high energy physics (HEP) Auger project and a protein conformation predictor using a molecular dynamic simulator. To evaluate the performance and volatility tolerance. we present experiment results for bag of tasks applications and message passing applications. We show that the system can tolerate massive failure and we discuss the performance of the node protection mechanism. Based on the XtremWeb project developments and evolutions, we will discuss the convergence between Global Computing systems and Grid. (C) 2004 Published by Elsevier B.V.;Global Computing systems belong to the class of large-scale distributed systems. Their properties high computational, storage and communication performance potentials, high resilience make them attractive in academia and industry as computing infrastructures in complement to more classical infrastructures such as clusters or supercomputers. However, generalizing the use of these systems in a multi-user and multi-parallel programming context involves finding solutions and providing mechanisms for many issues such as programming bag of tasks and message passing parallel applications, securing the application, the system itself and the computing nodes, deploying the systems for harnessing resources managed in different ways. In this paper, we present our research, often influenced by user demands, towards a Computational peer-to-peer system called XtremWeb. We describe (a) the architecture of the system and its motivations, (b) the parallel programming paradigms available in XtremWeb and how they are implemented, (c) the deployment issues and what mechanisms are used to harness simultaneously uncoordinated set of resources, and resources managed by batch schedulers and (d) the security issue and how we address, inside XtremWeb, the protection of the computing resources. We present two multi-parametric applications to be used in production: Aires belonging to the high energy physics (HEP) Auger project and a protein conformation predictor using a molecular dynamic simulator. To evaluate the performance and volatility tolerance, we present experiment results for bag of tasks applications and message passing applications. We show that the system can tolerate massive failure and we discuss the performance of the node protection mechanism. Based on the XtremWeb project developments and evolutions, we will discuss the convergence between Global Computing systems and Grid. © 2004 Published by Elsevier B.V.;},
  keywords = {Computer Science; Computer Science, Theory & Methods; Distributed, Parallel, and Cluster Computing; Global Computing systems; Hardware Architecture; Large-scale distributed systems; Science & Technology; Technology; XtremWeb},
  isbn     = {0167-739X},
  language = {English}
}

@article{Chaubey2019,
  author   = {Chaubey,Mrityunjay and DST-CIMS, Institute of science Banaras Hindu University Varanasi, India},
  year     = {2019},
  title    = {DESIGNING A TASK ALLOCATOR FRAMEWORK FOR DISTRIBUTED COMPUTING},
  journal  = {International journal of advanced research in computer science},
  volume   = {10},
  number   = {4},
  pages    = {52-58},
  abstract = {Software Frameworks attempt to capture and implement a software system architecture that is reusable. A frameworks, thus, is a semicode that needs to be customized for a particular reuse. The problem of finding an optimal task allocation in distributed computing system (DCS) is an NPhard. There are various task allocation algorithms and hence a Task Allocator may implement any of them. Any Task Allocator, hence will have many portions that can be reused to define and implement a Task Allocator. In distributed system a Task Allocation mechanism may be replaced by a new one if a standardized definition of a reusable system architecture for this purpose is available. This work attempts at formalizing a system architecture of a Task Allocator by proposing a framework for the purpose. Here we start the design methodology for OO software and identify the various parts of the software system architecture for task allocation. This effort finally results into a semicode framework. The interesting conclusions include , and the nature of framework definitions that need to be coded at the time of reuse. In this work OO design of various activities of task allocation process has been carried out as per the OO design methodology. To be objects have been identified the dynamic and functional modeling along with identification use cases, corresponding scenarios and data flow diagrams.},
  keywords = {Algorithms; Computer architecture; Computer networks; Design engineering; Distributed processing; Reuse; Software},
  isbn     = {0976-5697},
  language = {English}
}

@article{Ince2022,
  author   = {Ince,Muhammed N. and Gunay,Melih and Ledet,Joseph},
  year     = {2022},
  title    = {Lightweight distributed computing framework for orchestrating high performance computing and big data},
  journal  = {Elektrik : Turkish journal of electrical engineering \& computer sciences},
  volume   = {30},
  number   = {4},
  pages    = {1571-1585},
  abstract = {In recent years, the need for the ability to work remotely and subsequently the need for the availability of remote computer-based systems has increased substantially. This trend has seen a dramatic increase with the onset of the 2020 pandemic. Often local data is produced, stored, and processed in the cloud to remedy this flood of computation and storage needs. Historically, HPC (high performance computing) and the concept of big data have been utilized for the storage and processing of large data. However, both HPC and Hadoop can be utilized as solutions for analytical work, though the differences between these may not be obvious. Both use parallel processing techniques and offer options for data to be stored in either a centralized or distributed manner. Recent studies have focused on using a hybrid approach with both technologies. Therefore, the convergence between HPC and big data technologies can be filled with distributed computing machines at the layer described. This paper results from the motivation that there exists a necessity for a distributed computing framework that can scale from SOC (system on chip) boards to desktop computers and servers. For this purpose, in this article, we propose a distributed computing environment that can scale up to devices with heterogeneous architecture, where devices can set up clusters with resource-limited nodes and then run on top of. The solution can be thought of as a minimalist hybrid approach between HPC and big data. Within the scope of this study, not only the design of the proposed system is detailed, but also critical modules and subsystems are implemented as proof of concept.},
  keywords = {big data; Computer Science; Computer Science, Artificial Intelligence; Distributed and parallel computing; distributed programming; Engineering; Engineering, Electrical & Electronic; high performance computing; resource management; Science & Technology; Technology},
  isbn     = {1300-0632},
  language = {English}
}

@article{Korpela2012,
  author   = {Korpela,Eric J.},
  year     = {2012},
  title    = {SETI@home, BOINC, and Volunteer Distributed Computing},
  journal  = {Annual review of earth and planetary sciences},
  volume   = {40},
  number   = {1},
  pages    = {69-87},
  abstract = {Volunteer computing, also known as public-resource computing, is a form of distributed computing that relies on members of the public donating the processing power, Internet connection, and storage capabilities of their home computers. Projects that utilize this mode of distributed computation can potentially access millions of Internet-attached central processing units (CPUs) that provide PFLOPS (thousands of trillions of floating-point operations per second) of processing power. In addition, these projects can access the talents of the volunteers themselves. Projects span a wide variety of domains including astronomy, biochemistry, climatology, physics, and mathematics. This review provides an introduction to volunteer computing and some of the difficulties involved in its implementation. I describe the dominant infrastructure for volunteer computing in some depth and provide descriptions of a small number of projects as an illustration of the variety of projects that can be undertaken.;Volunteer computing, also known as public-resource computing, is a form of distributed computing that relies on members of the public donating the processing power, Internet connection, and storage capabilities of their home computers. Projects that utilize this mode of distributed computation can potentially access millions of Internet-attached central processing units (CPUs) that provide PFLOPS (thousands of trillions of floating-point operations per second) of processing power. In addition, these projects can access the talents of the volunteers themselves. Projects span a wide variety of domains including astronomy, biochemistry, climatology, physics, and mathematics. This review provides an introduction to volunteer computing and some of the difficulties involved in its implementation. I describe the dominant infrastructure for volunteer computing in some depth and provide descriptions of a small number of projects as an illustration of the variety of projects that can be undertaken. [PUBLICATION ABSTRACT];},
  keywords = {Astronomy; Astronomy & Astrophysics; Central processing units; Citizen science; Climatology; CPUs; Crowdsourcing; Distributed processing; Geology; Geosciences, Multidisciplinary; Grid computing; High-performance computing; Internet access; Physical Sciences; Public participation in science; Science & Technology},
  isbn     = {0084-6597},
  language = {English}
}

@article{Lucchese2010,
  author   = {Lucchese,C. and Mastroianni,C. and Orlando,S. and Talia,D.},
  year     = {2010},
  title    = {Mining@home: toward a public-resource computing framework for distributed data mining},
  journal  = {Concurrency and computation},
  volume   = {22},
  number   = {5},
  pages    = {658-682},
  abstract = {Several classes of scientific and commercial applications require the execution of a large number of independent tasks. One highly successful and low‐cost mechanism for acquiring the necessary computing power for these applications is the ‘public‐resource computing’, or ‘desktop Grid’ paradigm, which exploits the computational power of private computers. So far, this paradigm has not been applied to data mining applications for two main reasons. First, it is not straightforward to decompose a data mining algorithm into truly independent sub‐tasks. Second, the large volume of the involved data makes it difficult to handle the communication costs of a parallel paradigm. This paper introduces a general framework for distributed data mining applications called Mining@home. In particular, we focus on one of the main data mining problems: the extraction of closed frequent itemsets from transactional databases. We show that it is possible to decompose this problem into independent tasks, which however need to share a large volume of the data. We thus introduce a data‐intensive computing network, which adopts a P2P topology based on super peers with caching capabilities, aiming to support the dissemination of large amounts of information. Finally, we evaluate the execution of a pattern extraction task on such network. Copyright © 2009 John Wiley & Sons, Ltd.;Several classes of scientific and commercial applications require the execution of a large number of independent tasks. One highly successful and low‐cost mechanism for acquiring the necessary computing power for these applications is the ‘public‐resource computing’, or ‘desktop Grid’ paradigm, which exploits the computational power of private computers. So far, this paradigm has not been applied to data mining applications for two main reasons. First, it is not straightforward to decompose a data mining algorithm into truly independent sub‐tasks. Second, the large volume of the involved data makes it difficult to handle the communication costs of a parallel paradigm. This paper introduces a general framework for distributed data mining applications called Mining@home. In particular, we focus on one of the main data mining problems: the extraction of closed frequent itemsets from transactional databases. We show that it is possible to decompose this problem into independent tasks, which however need to share a large volume of the data. We thus introduce a data‐intensive computing network, which adopts a P2P topology based on super peers with caching capabilities, aiming to support the dissemination of large amounts of information. Finally, we evaluate the execution of a pattern extraction task on such network. Copyright © 2009 John Wiley & Sons, Ltd.;Several classes of scientific and commercial applications require the execution of a large number of independent tasks. One highly successful and low-cost mechanism for acquiring the necessary computing power for these applications is the 'public-resource computing', or 'desktop Grid' paradigm, which exploits the computational power of private computers. So far, this paradigm has not been applied to data mining applications for two main reasons. First, it is not straightforward to decompose a data mining algorithm into truly independent sub-tasks. Second, the large volume of the involved data makes it difficult to handle the communication costs of a parallel paradigm. This paper introduces a general framework for distributed data mining applications called Mining@home. In particular, we focus on one of the main data mining problems: the extraction of closed frequent itemsets from transactional databases. We show that it is possible to decompose this problem into independent tasks, which however need to share a large volume of the data. We thus introduce a data-intensive computing network, which adopts a P2P topology based on super peers with caching capabilities, aiming to support the dissemination of large amounts of information. Finally, we evaluate the execution of a pattern extraction task on such network. Copyright (c) 2009 John Wiley & Sons, Ltd.;},
  keywords = {Closed frequent itemsets; Computer Science; Computer Science, Software Engineering; Computer Science, Theory & Methods; Data mining; Desktop grids; Peer-to-peer computing; Public-resource computing; Science & Technology; Technology},
  isbn     = {1532-0626},
  language = {English}
}

@article{MaddahAli2014,
  author   = {Maddah-Ali,Mohammad A. and Niesen,Urs},
  year     = {2014},
  title    = {Fundamental Limits of Caching},
  journal  = {IEEE transactions on information theory},
  volume   = {60},
  number   = {5},
  pages    = {2856-2867},
  abstract = {Caching is a technique to reduce peak traffic rates by prefetching popular content into memories at the end users. Conventionally, these memories are used to deliver requested content in part from a locally cached copy rather than through the network. The gain offered by this approach, which we term local caching gain, depends on the local cache size (i.e., the memory available at each individual user). In this paper, we introduce and exploit a second, global, caching gain not utilized by conventional caching schemes. This gain depends on the aggregate global cache size (i.e., the cumulative memory available at all users), even though there is no cooperation among the users. To evaluate and isolate these two gains, we introduce an information-theoretic formulation of the caching problem focusing on its basic structure. For this setting, we propose a novel coded caching scheme that exploits both local and global caching gains, leading to a multiplicative improvement in the peak rate compared with previously known schemes. In particular, the improvement can be on the order of the number of users in the network. In addition, we argue that the performance of the proposed scheme is within a constant factor of the information-theoretic optimum for all values of the problem parameters. [PUBLICATION ABSTRACT];Caching is a technique to reduce peak traffic rates by prefetching popular content into memories at the end users. Conventionally, these memories are used to deliver requested content in part from a locally cached copy rather than through the network. The gain offered by this approach, which we term local caching gain, depends on the local cache size (i.e., the memory available at each individual user). In this paper, we introduce and exploit a second, global, caching gain not utilized by conventional caching schemes. This gain depends on the aggregate global cache size (i.e., the cumulative memory available at all users), even though there is no cooperation among the users. To evaluate and isolate these two gains, we introduce an information-theoretic formulation of the caching problem focusing on its basic structure. For this setting, we propose a novel coded caching scheme that exploits both local and global caching gains, leading to a multiplicative improvement in the peak rate compared with previously known schemes. In particular, the improvement can be on the order of the number of users in the network. In addition, we argue that the performance of the proposed scheme is within a constant factor of the information-theoretic optimum for all values of the problem parameters.;},
  keywords = {Aggregates; Cache memory; Caching; coded caching; Codes; Computer Science; Computer Science, Information Systems; Constants; content distribution; End users; Engineering; Engineering, Electrical & Electronic; Gain; Information theory; Multicast communication; Networks; Optimization; Prefetching; Radio frequency; Reproduction; Science & Technology; Servers; Technology; Upper bound},
  isbn     = {0018-9448},
  language = {English}
}

@article{Li2017,
  author   = {Li,Songze and Yu,Qian and Maddah-Ali,Mohammad A. and Avestimehr,A. S.},
  year     = {2017},
  title    = {A Scalable Framework for Wireless Distributed Computing},
  journal  = {IEEE/ACM transactions on networking},
  volume   = {25},
  number   = {5},
  pages    = {2643-2654},
  abstract = {We consider a wireless distributed computing system, in which multiple mobile users, connected wirelessly through an access point, collaborate to perform a computation task. In particular, users communicate with each other via the access point to exchange their locally computed intermediate computation results, which is known as data shuffling. We propose a scalable framework for this system, in which the required communication bandwidth for data shuffling does not increase with the number of users in the network. The key idea is to utilize a particular repetitive pattern of placing the data set ( thus a particular repetitive pattern of intermediate computations), in order to provide the coding opportunities at both the users and the access point, which reduce the required uplink communication bandwidth from users to the access point and the downlink communication bandwidth from access point to users by factors that grow linearly with the number of users. We also demonstrate that the proposed data set placement and coded shuffling schemes are optimal (i.e., achieve the minimum required shuffling load) for both a centralized setting and a decentralized setting, by developing tight information-theoretic lower bounds.},
  keywords = {Bandwidths; coding; Communication; Communication system security; Communications systems; Computation; Computer networks; Computer Science; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Distributed computing; Distributed processing; Downlink; edge computing; Encoding; Engineering; Engineering, Electrical & Electronic; Information theory; Lower bounds; Mobile communication; scalability; Science & Technology; Technology; Telecommunications; Uplink; User requirements; Wireless access points; Wireless communication; Wireless distributed computing},
  isbn     = {1063-6692},
  language = {English}
}

@article{Mengistu2020,
  author   = {Mengistu,Tessema M. and Che,Dunren},
  year     = {2020},
  title    = {Survey and taxonomy of volunteer computing},
  journal  = {ACM computing surveys},
  volume   = {52},
  number   = {3},
  pages    = {1-35},
  abstract = {Volunteer Computing is a kind of distributed computing that harnesses the aggregated spare computing resources of volunteer devices. It provides a cheaper and greener alternative computing infrastructure that can complement the dedicated, centralized, and expensive data centres. The aggregated idle computing resources of devices ranging from desktop computers to routers and smart TVs are being utilized to provide the much needed computing infrastructure for compute intensive tasks such as scientific simulations and big data analysis. However, the use of Volunteer Computing is still dominated by scientific applications and only a very small fraction of the potential volunteer nodes are participating. This article provides a comprehensive survey of Volunteer Computing, covering key technical and operational issues such as security, task distribution, resource management, and incentive models. The article also presents a taxonomy of Volunteer Computing systems, together with discussions of the characteristics of specific systems in each category. To harness the full potentials of Volunteer Computing and make it a reliable alternative computing infrastructure for general applications, we need to improve the existing techniques and device new mechanisms. Thus, this article also sheds light on important issues regarding the future research and development of Volunteer Computing systems with the aim of making them a viable alternative computing infrastructure.},
  keywords = {Computer networks; Computer Science; Computer Science, Theory & Methods; Computer simulation; Data analysis; Data centers; Desktop grid; Distributed processing; Edge/fog computing; Harnesses; Infrastructure; Mobile volunteer computing; P2P volunteer computing; Personal computers; R&D; Research & development; Resource management; Routers; Science & Technology; Taxonomy; Technology; Volunteer cloud computing; Volunteer computing},
  isbn     = {0360-0300},
  language = {English}
}

@article{Zhang2012,
  author   = {Zhang,Yanfeng and Gao,Qixin and Gao,Lixin and Wang,Cuirong},
  year     = {2012},
  title    = {iMapReduce: A Distributed Computing Framework for Iterative Computation},
  journal  = {Journal of grid computing},
  volume   = {10},
  number   = {1},
  pages    = {47-68},
  abstract = {Iterative computation is pervasive in many applications such as data mining, web ranking, graph analysis, online social network analysis, and so on. These iterative applications typically involve massive data sets containing millions or billions of data records. This poses demand of distributed computing frameworks for processing massive data sets on a cluster of machines. MapReduce is an example of such a framework. However, MapReduce lacks built-in support for iterative process that requires to parse data sets iteratively. Besides specifying MapReduce jobs, users have to write a driver program that submits a series of jobs and performs convergence testing at the client. This paper presents iMapReduce, a distributed framework that supports iterative processing. iMapReduce allows users to specify the iterative computation with the separated map and reduce functions, and provides the support of automatic iterative processing within a single job. More importantly, iMapReduce significantly improves the performance of iterative implementations by (1) reducing the overhead of creating new MapReduce jobs repeatedly, (2) eliminating the shuffling of static data, and (3) allowing asynchronous execution of map tasks. We implement an iMapReduce prototype based on Apache Hadoop, and show that iMapReduce can achieve up to 5 times speedup over Hadoop for implementing iterative algorithms.},
  keywords = {Computation; Computer networks; Computer Science; Computer Science, Information Systems; Computer Science, Theory & Methods; Data mining; Datasets; Distributed computing framework; Distributed processing; Hadoop; iMapReduce; Iterative algorithms; Iterative computation; Iterative methods; Management of Computing and Information Systems; Massive data points; Network analysis; Performance enhancement; Processor Architectures; Science & Technology; Social networks; Technology; User Interfaces and Human Computer Interaction},
  isbn     = {1570-7873},
  language = {English}
}